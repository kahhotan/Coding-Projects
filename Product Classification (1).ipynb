{"cells":[{"cell_type":"code","source":["#We will be building a text classifier using various techniques\n#As we are using Databricks Community version, we are limited by resouces, and as such, fail to run BERT and ELMO\n#We will be utilizing MLFlow to track the various performances of these techniques\n#Dataset is from National Data Science Challenge on Kaggle"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48c15ae4-7fad-447d-9791-c154cb2bc9e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["train_df = spark.read.parquet('dbfs:/user/hive/warehouse/products_train')\ndisplay(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1935f85-8cd2-4f02-9880-8cc142f25716"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndisplay(train_df.groupBy(\"Category\") \\\n            .count() \\\n            .orderBy(col(\"count\").desc()))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b08f2f8-b3ec-4d96-9a4f-4e00c706a2b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[3,81250],[18,56598],[5,55279],[4,42688],[25,33922],[26,33845],[35,30590],[32,29754],[1,28670],[31,27836],[12,21782],[20,19978],[41,19312],[27,16199],[22,15225],[34,14718],[19,13493],[7,11668],[2,11544],[42,10502],[21,10483],[9,8106],[28,6493],[43,6147],[8,6048],[33,4821],[38,4703],[24,4251],[11,4108],[0,3772],[29,3338],[13,2982],[17,2765],[14,2735],[37,2272],[16,2228],[45,2195],[6,2047],[23,1671],[30,1441],[10,1077],[36,1028],[47,945],[44,920],[39,697],[46,684],[15,599],[49,573],[53,418],[48,416],[51,404],[40,327],[54,300],[50,282],[56,170],[55,151],[52,117],[57,48]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Category","type":"\"integer\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Category</th><th>count</th></tr></thead><tbody><tr><td>3</td><td>81250</td></tr><tr><td>18</td><td>56598</td></tr><tr><td>5</td><td>55279</td></tr><tr><td>4</td><td>42688</td></tr><tr><td>25</td><td>33922</td></tr><tr><td>26</td><td>33845</td></tr><tr><td>35</td><td>30590</td></tr><tr><td>32</td><td>29754</td></tr><tr><td>1</td><td>28670</td></tr><tr><td>31</td><td>27836</td></tr><tr><td>12</td><td>21782</td></tr><tr><td>20</td><td>19978</td></tr><tr><td>41</td><td>19312</td></tr><tr><td>27</td><td>16199</td></tr><tr><td>22</td><td>15225</td></tr><tr><td>34</td><td>14718</td></tr><tr><td>19</td><td>13493</td></tr><tr><td>7</td><td>11668</td></tr><tr><td>2</td><td>11544</td></tr><tr><td>42</td><td>10502</td></tr><tr><td>21</td><td>10483</td></tr><tr><td>9</td><td>8106</td></tr><tr><td>28</td><td>6493</td></tr><tr><td>43</td><td>6147</td></tr><tr><td>8</td><td>6048</td></tr><tr><td>33</td><td>4821</td></tr><tr><td>38</td><td>4703</td></tr><tr><td>24</td><td>4251</td></tr><tr><td>11</td><td>4108</td></tr><tr><td>0</td><td>3772</td></tr><tr><td>29</td><td>3338</td></tr><tr><td>13</td><td>2982</td></tr><tr><td>17</td><td>2765</td></tr><tr><td>14</td><td>2735</td></tr><tr><td>37</td><td>2272</td></tr><tr><td>16</td><td>2228</td></tr><tr><td>45</td><td>2195</td></tr><tr><td>6</td><td>2047</td></tr><tr><td>23</td><td>1671</td></tr><tr><td>30</td><td>1441</td></tr><tr><td>10</td><td>1077</td></tr><tr><td>36</td><td>1028</td></tr><tr><td>47</td><td>945</td></tr><tr><td>44</td><td>920</td></tr><tr><td>39</td><td>697</td></tr><tr><td>46</td><td>684</td></tr><tr><td>15</td><td>599</td></tr><tr><td>49</td><td>573</td></tr><tr><td>53</td><td>418</td></tr><tr><td>48</td><td>416</td></tr><tr><td>51</td><td>404</td></tr><tr><td>40</td><td>327</td></tr><tr><td>54</td><td>300</td></tr><tr><td>50</td><td>282</td></tr><tr><td>56</td><td>170</td></tr><tr><td>55</td><td>151</td></tr><tr><td>52</td><td>117</td></tr><tr><td>57</td><td>48</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(train_df.groupBy(\"Main_Category\") \\\n            .count() \\\n            .orderBy(col(\"count\").desc()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2926585d-d7c0-4511-b83c-a630d4827548"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["beauty",286583],["fashion",219702],["mobile",160330]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Main_Category","type":"\"string\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Main_Category</th><th>count</th></tr></thead><tbody><tr><td>beauty</td><td>286583</td></tr><tr><td>fashion</td><td>219702</td></tr><tr><td>mobile</td><td>160330</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["test_df = spark.read.parquet('dbfs:/user/hive/warehouse/products_test')\n\ndisplay(test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e7fc563-9365-4719-8fd0-e876f1c6f06d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(test_df.groupBy(\"Main_Category\") \\\n            .count() \\\n            .orderBy(col(\"count\").desc()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ce0d6a7-8cd4-478d-931c-4cd98bf86e4e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["beauty",76545],["fashion",55440],["mobile",40417]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Main_Category","type":"\"string\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Main_Category</th><th>count</th></tr></thead><tbody><tr><td>beauty</td><td>76545</td></tr><tr><td>fashion</td><td>55440</td></tr><tr><td>mobile</td><td>40417</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.base import *\n\nfrom pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer,IndexToString\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# use sklearn to evalute the results on test dataset\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bffc9cdc-1f8c-4296-a7bb-0628d5443c4c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Spark NLP Pipeline\n# convert text column to spark nlp document\ndocument_assembler = DocumentAssembler()\\\n  .setInputCol(\"title\")\\\n  .setOutputCol(\"document\")\n\n# convert document to array of tokens\ntokenizer = Tokenizer() \\\n  .setInputCols([\"document\"]) \\\n  .setOutputCol(\"token\")\n \n# clean tokens \nnormalizer = Normalizer() \\\n    .setInputCols([\"token\"]) \\\n    .setOutputCol(\"normalized\")\n\n# remove stopwords\nstopwords_cleaner = StopWordsCleaner()\\\n  .setInputCols(\"normalized\")\\\n  .setOutputCol(\"cleanTokens\")\\\n  .setCaseSensitive(False)\n\n# stems tokens to bring it to root form\nstemmer = Stemmer() \\\n  .setInputCols([\"cleanTokens\"]) \\\n  .setOutputCol(\"stem\")\n\n# Convert custom document structure to array of tokens.\nfinisher = Finisher() \\\n  .setInputCols([\"stem\"]) \\\n  .setOutputCols([\"token_features\"]) \\\n  .setOutputAsArray(True) \\\n  .setCleanAnnotations(False)\n\n# To generate Term Frequency\nhashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\", numFeatures=1000)\n\n# To generate Inverse Document Frequency\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n\n# convert labels (string) to integers. Easy to process compared to string.\nlabel_stringIdx = StringIndexer(inputCol = \"Main_Category\", outputCol = \"label\")\n\n# To convert index(integer) to corresponding class labels\nlabel_to_stringIdx = IndexToString(inputCol=\"label\", outputCol=\"class\")\n\nuse = UniversalSentenceEncoder.pretrained()\\\n .setInputCols([\"document\"])\\\n .setOutputCol(\"sentence_embeddings\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"233cec0b-5838-4c97-9cf4-d7223e3ed216"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"tfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[OK!]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["tfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[OK!]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Prepare Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c8b383b-68e8-4f5c-96a3-654c318931f7"}}},{"cell_type":"code","source":["pipeline = Pipeline(\n  stages = [document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner, \n            stemmer, \n            finisher,\n            hashingTF,\n            idf,\n            label_stringIdx])\n\n# fit pipeline to training data\ndf_fit = pipeline.fit(train_df)\ntrainData = df_fit.transform(train_df)\n\n# fit pipeline to testing data\ndf_fit = pipeline.fit(test_df)\ntestData = df_fit.transform(test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85e41636-b735-4de6-9f93-8fe7355eeadf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(\"Training Dataset Count: \" + str(trainData.count()))\nprint(\"Test Dataset Count: \" + str(testData.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fef3547f-887a-411a-811f-10f89a5d697d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Training Dataset Count: 666615\nTest Dataset Count: 172402\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Training Dataset Count: 666615\nTest Dataset Count: 172402\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Train a Logistic Regression Classifier"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ece6988c-d6e7-476d-b525-d38746a3eda3"}}},{"cell_type":"code","source":["# define a simple Multinomial logistic regression model. \nlr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0.0)\nlrModel = lr.fit(trainData)\nlr_preds = lrModel.transform(testData)\n\npreds_df = lr_preds.select('label', 'prediction').toPandas()\n\nprint(classification_report(preds_df.label, preds_df.prediction))\nprint('Accuracy Score: ' + str(accuracy_score(preds_df.label, preds_df.prediction)))\nprint('F1 Score: ' + str(f1_score(preds_df.label, preds_df.prediction, average='weighted')))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4861098c-9577-4fa4-8732-fa856cf50a9b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"              precision    recall  f1-score   support\n\n         0.0       0.97      0.99      0.98     76545\n         1.0       0.99      0.98      0.99     55440\n         2.0       0.98      0.95      0.97     40417\n\n    accuracy                           0.98    172402\n   macro avg       0.98      0.98      0.98    172402\nweighted avg       0.98      0.98      0.98    172402\n\nAccuracy Score: 0.9791243721070522\nF1 Score: 0.9791119267671589\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["              precision    recall  f1-score   support\n\n         0.0       0.97      0.99      0.98     76545\n         1.0       0.99      0.98      0.99     55440\n         2.0       0.98      0.95      0.97     40417\n\n    accuracy                           0.98    172402\n   macro avg       0.98      0.98      0.98    172402\nweighted avg       0.98      0.98      0.98    172402\n\nAccuracy Score: 0.9791243721070522\nF1 Score: 0.9791119267671589\n"]}}],"execution_count":0},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\nevaluator.evaluate(lr_preds)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"528ef7c3-5b62-4c00-ac2a-7d9119cb8933"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: 0.9791119267671589","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: 0.9791119267671589"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Naive Bayes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dbf1cd2-c6ed-4d2a-a2cd-b3f0f55f82b7"}}},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\n\nnb = NaiveBayes(smoothing=1)\nnbModel = nb.fit(trainData)\nnb_preds = nbModel.transform(testData)\n\npreds_df = nb_preds.select('label', 'prediction').toPandas()\n\nprint(classification_report(preds_df.label, preds_df.prediction))\nprint('Accuracy Score: ' + str(accuracy_score(preds_df.label, preds_df.prediction)))\nprint('F1 Score: ' + str(f1_score(preds_df.label, preds_df.prediction, average='weighted')))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1478b7d4-46a0-40fa-89b5-0b8431c4ee2e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"              precision    recall  f1-score   support\n\n         0.0       0.98      0.97      0.97     76545\n         1.0       0.97      0.99      0.98     55440\n         2.0       0.97      0.97      0.97     40417\n\n    accuracy                           0.97    172402\n   macro avg       0.97      0.98      0.97    172402\nweighted avg       0.97      0.97      0.97    172402\n\nAccuracy Score: 0.9748262781174233\nF1 Score: 0.9748130360294088\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["              precision    recall  f1-score   support\n\n         0.0       0.98      0.97      0.97     76545\n         1.0       0.97      0.99      0.98     55440\n         2.0       0.97      0.97      0.97     40417\n\n    accuracy                           0.97    172402\n   macro avg       0.97      0.98      0.97    172402\nweighted avg       0.97      0.97      0.97    172402\n\nAccuracy Score: 0.9748262781174233\nF1 Score: 0.9748130360294088\n"]}}],"execution_count":0},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\nevaluator.evaluate(nb_preds)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d1c14fd-56aa-498a-b36d-d7385eecef0e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[13]: 0.9748130360294089","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[13]: 0.9748130360294089"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Random Forest Classifier"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0467882-f295-4e2f-a084-f9e8ea2f5672"}}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(labelCol=\"label\", \\\n                            featuresCol=\"features\", \\\n                            numTrees = 100, \\\n                            maxDepth = 4, \\\n                            maxBins = 32)\n\n\nrfModel = rf.fit(trainData)\nrf_preds = rfModel.transform(testData)\n\npreds_df = rf_preds.select('label', 'prediction').toPandas()\n\nprint(classification_report(preds_df.label, preds_df.prediction))\nprint('Accuracy Score: ' + str(accuracy_score(preds_df.label, preds_df.prediction)))\nprint('F1 Score: ' + str(f1_score(preds_df.label, preds_df.prediction, average='weighted')))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cb1dec3-3330-40ed-91ad-167784648776"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"              precision    recall  f1-score   support\n\n         0.0       0.66      1.00      0.80     76545\n         1.0       1.00      0.81      0.90     55440\n         2.0       1.00      0.28      0.44     40417\n\n    accuracy                           0.77    172402\n   macro avg       0.89      0.70      0.71    172402\nweighted avg       0.85      0.77      0.74    172402\n\nAccuracy Score: 0.7713947634018167\nF1 Score: 0.7444652651123748\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["              precision    recall  f1-score   support\n\n         0.0       0.66      1.00      0.80     76545\n         1.0       1.00      0.81      0.90     55440\n         2.0       1.00      0.28      0.44     40417\n\n    accuracy                           0.77    172402\n   macro avg       0.89      0.70      0.71    172402\nweighted avg       0.85      0.77      0.74    172402\n\nAccuracy Score: 0.7713947634018167\nF1 Score: 0.7444652651123748\n"]}}],"execution_count":0},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\nevaluator.evaluate(rf_preds)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20198c31-01b3-46da-942a-202e47a04b9d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[15]: 0.7444652651123747","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[15]: 0.7444652651123747"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Cross-Validation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f21e3a1-58d6-4382-8c8f-3e0856743e67"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nlr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n             .build())\n\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, \\\n                    estimatorParamMaps=paramGrid, \\\n                    evaluator=evaluator, \\\n                    numFolds=5)\ncvModel = cv.fit(trainData)\n\npredictions = cvModel.transform(testData)\n\n# Evaluate best model\nevaluator.evaluate(predictions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97ff69ed-a8cf-4ebe-9798-510878791af2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\nOut[16]: 0.9831521089877477","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\nOut[16]: 0.9831521089877477"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Train a Deep Learning Model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20dea6e2-33a1-4aac-a665-7232aba3a71a"}}},{"cell_type":"markdown","source":["### DL + WordEmbedding"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d01214b8-0749-4f7b-8b4e-32bbfbbc48d2"}}},{"cell_type":"code","source":["# bring token to lemma form\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n  .setInputCols([\"cleanTokens\"]) \\\n  .setOutputCol(\"lemma\")\n\nword_embeddings = WordEmbeddingsModel().pretrained() \\\n  .setInputCols(['document','lemma']) \\\n  .setOutputCol('embeddings') \\\n  .setCaseSensitive(False)\n\nembeddingsSentence = SentenceEmbeddings() \\\n  .setInputCols(['document', 'embeddings']) \\\n  .setOutputCol('sentence_embeddings') \\\n  .setPoolingStrategy('Average')\n\n# the classes/labels/categories are in category column\nclassifierdl = ClassifierDLApproach()\\\n  .setInputCols([\"sentence_embeddings\"])\\\n  .setOutputCol(\"class\")\\\n  .setLabelColumn(\"Main_Category\")\\\n  .setMaxEpochs(5) \\\n  .setEnableOutputLogs(True)\n\n# classifierdl pipeline\nclf_pipeline = Pipeline(\n  stages = [document_assembler,\n            tokenizer,\n            normalizer, \n            stopwords_cleaner,\n            lemma, \n            word_embeddings,\n            embeddingsSentence,\n            classifierdl])\n\n# fit the pipeline on training data\nclfModel = clf_pipeline.fit(train_df)\n\n# fit the pipeline on training data\nclf_preds = clfModel.transform(test_df)\n\npreds_df = clf_preds.select('Main_Category', 'class.result').toPandas()\npreds_df['result'] = preds_df['result'].apply(lambda x: x[0])\n\nprint(classification_report(preds_df['Main_Category'], preds_df.result))\nprint('Accuracy Score: ' + str(accuracy_score(preds_df['Main_Category'], preds_df.result)))\nprint('F1 Score: ' + str(f1_score(preds_df['Main_Category'], preds_df.result, average='weighted')))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f51c3353-e3f4-44a8-999c-ba91f99b2eaf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[OK!]\nglove_100d download started this may take some time.\nApproximate size to download 145.3 MB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[OK!]\n              precision    recall  f1-score   support\n\n      beauty       0.99      1.00      1.00     76545\n     fashion       1.00      1.00      1.00     55440\n      mobile       1.00      0.99      0.99     40417\n\n    accuracy                           1.00    172402\n   macro avg       1.00      1.00      1.00    172402\nweighted avg       1.00      1.00      1.00    172402\n\nAccuracy Score: 0.995655502836394\nF1 Score: 0.9956539009284682\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[OK!]\nglove_100d download started this may take some time.\nApproximate size to download 145.3 MB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[OK!]\n              precision    recall  f1-score   support\n\n      beauty       0.99      1.00      1.00     76545\n     fashion       1.00      1.00      1.00     55440\n      mobile       1.00      0.99      0.99     40417\n\n    accuracy                           1.00    172402\n   macro avg       1.00      1.00      1.00    172402\nweighted avg       1.00      1.00      1.00    172402\n\nAccuracy Score: 0.995655502836394\nF1 Score: 0.9956539009284682\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### DL + BERT Embedding"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d226001e-401f-4305-bbbb-2baa9130a3cd"}}},{"cell_type":"code","source":["\n #BERT Embeddings\nword_embeddings = BertEmbeddings \\\n  .pretrained('bert_base_cased', 'en') \\\n  .setInputCols(['document','cleanTokens']) \\\n  .setOutputCol('embeddings')\n\nembeddingsSentence = SentenceEmbeddings() \\\n  .setInputCols(['document', 'embeddings']) \\\n  .setOutputCol('sentence_embeddings') \\\n  .setPoolingStrategy('Average')\n\n# the classes/labels/categories are in category column\nclassifierdl = ClassifierDLApproach()\\\n  .setInputCols([\"sentence_embeddings\"])\\\n  .setOutputCol(\"class\")\\\n  .setLabelColumn(\"Main_Category\")\\\n  .setMaxEpochs(5) \\\n  .setEnableOutputLogs(True)\n\nbert_pipeline = Pipeline(\n  stages = [document_assembler,\n            tokenizer,\n            normalizer, \n            stopwords_cleaner,\n            word_embeddings,\n            embeddingsSentence,\n            classifierdl])\n\n# fit the pipeline on training data\nbertModel = bert_pipeline.fit(train_df)\n\n# fit the pipeline on training data\nbert_preds = bertModel.transform(test_df)\n\npreds_df = bert_preds.select('Main_Category', 'class.result').toPandas()\npreds_df['result'] = preds_df['result'].apply(lambda x: x[0])\n\nprint(classification_report(preds_df['Main_Category'], preds_df.result))\nprint('Accuracy Score: ' + str(accuracy_score(preds_df['Main_Category'], preds_df.result)))\nprint('F1 Score: ' + str(f1_score(preds_df['Main_Category'], preds_df.result, average='weighted')))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acf52487-05c3-45af-aea5-941fbb5db6af"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### DL + Elmo Embeddings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6acd8ce5-a1aa-45c0-bf5c-f97eabf302c9"}}},{"cell_type":"code","source":[" #elmo Embeddings\nword_embeddings = ElmoEmbeddings \\\n  .pretrained('elmo', 'en') \\\n  .setInputCols(['document','lemma']) \\\n  .setOutputCol('embeddings')\n\nelmo_pipeline = Pipeline(\n  stages = [document_assembler,\n            tokenizer,\n            normalizer, \n            stopwords_cleaner,\n            lemma, \n            word_embeddings,\n            embeddingsSentence,\n            classifierdl])\n\n# fit the pipeline on training data\nelmoModel = elmo_pipeline.fit(train_df)\n\n# fit the pipeline on training data\nelmo_preds = elmoModel.transform(test_df)\n\npreds_df = elmo_preds.select('Main_Category', 'class.result').toPandas()\npreds_df['result'] = preds_df['result'].apply(lambda x: x[0])\n\nprint(classification_report(preds_df['Main_Category'], preds_df.result))\nprint('Accuracy Score: ' + str(accuracy_score(preds_df['Main_Category'], preds_df.result)))\nprint('F1 Score: ' + str(f1_score(preds_df['Main_Category'], preds_df.result, average='weighted')))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09c9fe25-a724-4320-8e61-0ae0e0c9030d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Export Pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5cd750c-697e-44f6-997f-b8f6d04b3519"}}},{"cell_type":"code","source":["clfModel.write().overwrite().save('./clfmodel')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0773358-557c-40e5-924c-a2c5867e7064"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from dbmlModelExport import ModelExport\n\n remove old model file, if needed.\ndbutils.fs.rm( \"/tmp/ml_python_model_export/classifierdl_pipeline\", recurse=True)\n\nModelExport.exportModel(clfModel, \"/tmp/ml_python_model_export/classifierdl_pipeline\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05a1ecb1-0328-4b11-882d-b4180a61db84"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[" define the Logistic Regression nlp pipeline\nlr_pipeline = Pipeline(\n  stages = [document_assembler, \n            tokenizer,\n            normalizer,\n           stopwords_cleaner, \n            stemmer, \n            finisher,\n            hashingTF,\n            idf,\n            label_stringIdx,\n            lr,\n            label_to_stringIdx])\n\n# fit the pipeline on training data\nlrModel = lr_pipeline.fit(train_df)\n\n# fit the pipeline on training data\nlr_preds = lrModel.transform(test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c109a59d-e0d6-4beb-a928-0ec4c255bf00"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Product Classification","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1812617809057434}},"nbformat":4,"nbformat_minor":0}
